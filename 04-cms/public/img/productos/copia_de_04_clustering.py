# -*- coding: utf-8 -*-
"""Copia de 04 Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yUFkFNqrxV-n7HoKM3dJIAiizBEvJTfP

# Ciencia de Datos y Machine Learning (UNCP)
## Alumno: John Eduardo Arroyo Bahamonde
### Tarea 13

# Configuración
"""

import sys

assert sys.version_info >= (3, 7)

from packaging import version
import sklearn

assert version.parse(sklearn.__version__) >= version.parse("1.0.1")

import matplotlib.pyplot as plt

plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)

"""And let's create the `images/unsupervised_learning` folder (if it doesn't already exist), and define the `save_fig()` function which is used through this notebook to save the figures in high-res for the book:"""

from pathlib import Path

IMAGES_PATH = Path() / "images" / "unsupervised_learning"
IMAGES_PATH.mkdir(parents=True, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = IMAGES_PATH / f"{fig_id}.{fig_extension}"
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

"""# Clustering (Agrupamiento)

**Classification _vs_ Clustering**
"""

# extra code – this cell generates and saves Figure 9–1

import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

data = load_iris()
X = data.data
y = data.target
data.target_names

plt.figure(figsize=(9, 3.5))

plt.subplot(121)
plt.plot(X[y==0, 2], X[y==0, 3], "yo", label="Iris setosa")
plt.plot(X[y==1, 2], X[y==1, 3], "bs", label="Iris versicolor")
plt.plot(X[y==2, 2], X[y==2, 3], "g^", label="Iris virginica")
plt.xlabel("Petal length")
plt.ylabel("Petal width")
plt.grid()
plt.legend()

plt.subplot(122)
plt.scatter(X[:, 2], X[:, 3], c="k", marker=".")
plt.xlabel("Petal length")
plt.tick_params(labelleft=False)
plt.gca().set_axisbelow(True)
plt.grid()

save_fig("classification_vs_clustering_plot")
plt.show()

"""**Nota**: la siguiente celda muestra cómo un modelo de mezcla gaussiana (que se explica más adelante en este capítulo) puede separar bastante bien estos clústeres utilizando las cuatro características: longitud y anchura de los pétalos, y longitud y anchura de los sépalos. Este código asigna cada clúster a una clase. En lugar de codificar la asignación de forma rígida, el código selecciona la clase más común para cada clúster utilizando la función `scipy.stats.mode()`:"""

# extra code

import numpy as np
from scipy import stats
from sklearn.mixture import GaussianMixture

y_pred = GaussianMixture(n_components=3, random_state=42).fit(X).predict(X)

mapping = {}
for class_id in np.unique(y):
    mode, _ = stats.mode(y_pred[y==class_id])
    mapping[mode] = class_id

y_pred = np.array([mapping[cluster_id] for cluster_id in y_pred])

plt.plot(X[y_pred==0, 2], X[y_pred==0, 3], "yo", label="Cluster 1")
plt.plot(X[y_pred==1, 2], X[y_pred==1, 3], "bs", label="Cluster 2")
plt.plot(X[y_pred==2, 2], X[y_pred==2, 3], "g^", label="Cluster 3")
plt.xlabel("Petal length")
plt.ylabel("Petal width")
plt.legend(loc="upper left")
plt.grid()
plt.show()

"""¿Cuál es la proporción de plantas de iris que hemos asignado al grupo correcto?"""

(y_pred==y).sum() / len(y_pred)

"""## K-Means

**Fit and predict**

Entrenemos un agrupador K-Means en un conjunto de datos si son blobs. Intentará encontrar el centro de cada blob y asignar cada instancia al blob más cercano:
"""

from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# extra code – the exact arguments of make_blobs() are not important
blob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8],
                         [-2.8,  2.8], [-2.8,  1.3]])
blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])
X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,
                  random_state=7) #7

k = 5
kmeans = KMeans(n_clusters=k, n_init=10, random_state=9)
y_pred = kmeans.fit_predict(X)

"""Ahora vamos a representarlos gráficamente:"""

# extra code – this cell generates and saves Figure 9–2

def plot_clusters(X, y=None):
    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)
    plt.xlabel("$x_1$")
    plt.ylabel("$x_2$", rotation=0)

plt.figure(figsize=(8, 4))
plot_clusters(X)
plt.gca().set_axisbelow(True)
plt.grid()
save_fig("blobs_plot")
plt.show()

"""Cada instancia se asignó a uno de los 5 clústeres:"""

y_pred

y_pred is kmeans.labels_

"""Y se estimaron los siguientes 5 _centroides_ (es decir, centros de agrupación):"""

kmeans.cluster_centers_

"""Tenga en cuenta que la instancia `KMeans` conserva las etiquetas de las instancias con las que se entrenó. Aunque resulte algo confuso, en este contexto, la _labels_ de una instancia es el índice del clúster al que se asigna dicha instancia (no son objetivos, son predicciones):"""

kmeans.labels_

"""Por supuesto, podemos predecir las etiquetas de nuevas instancias:"""

import numpy as np

X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
kmeans.predict(X_new)

"""**Decision Boundaries (límites de decisión)**

Trazemos los límites de decisión del modelo. Esto nos da un _diagrama de Voronoi_:
"""

# extra code – this cell generates and saves Figure 9–3

def plot_data(X):
    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)

def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):
    if weights is not None:
        centroids = centroids[weights > weights.max() / 10]
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='o', s=35, linewidths=8,
                color=circle_color, zorder=10, alpha=0.9)
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='x', s=2, linewidths=12,
                color=cross_color, zorder=11, alpha=1)

def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,
                             show_xlabels=True, show_ylabels=True):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                         np.linspace(mins[1], maxs[1], resolution))
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                cmap="Pastel2")
    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                linewidths=1, colors='k')
    plot_data(X)
    if show_centroids:
        plot_centroids(clusterer.cluster_centers_)

    if show_xlabels:
        plt.xlabel("$x_1$")
    else:
        plt.tick_params(labelbottom=False)
    if show_ylabels:
        plt.ylabel("$x_2$", rotation=0)
    else:
        plt.tick_params(labelleft=False)

plt.figure(figsize=(8, 4))
plot_decision_boundaries(kmeans, X)
save_fig("voronoi_plot")
plt.show()

"""¡No está mal! Es probable que algunas de las instancias cercanas a los bordes se hayan asignado al clúster equivocado, pero en general se ve bastante bien.

**Hard Clustering _vs_ Soft Clustering**

En lugar de elegir arbitrariamente el clúster más cercano para cada instancia, lo que se denomina «agrupamiento duro», podría ser mejor medir la distancia de cada instancia a los cinco centroides. Esto es lo que hace el método `transform()`:
"""

kmeans.transform(X_new).round(2)

"""Puedes verificar que esta es efectivamente la distancia euclidiana entre cada instancia y cada centroide:"""

# extra code
np.linalg.norm(np.tile(X_new, (1, k)).reshape(-1, k, 2)
               - kmeans.cluster_centers_, axis=2).round(2)

"""### The K-Means Algorithm

El algoritmo K-Means es uno de los algoritmos de agrupamiento más rápidos y también uno de los más sencillos:
  * Primero se inicializan $k$ centroides aleatoriamente: por ejemplo, se eligen aleatoriamente $k$ instancias distintas del conjunto de datos y se colocan los centroides en sus ubicaciones.
  * Se repite hasta la convergencia (es decir, hasta que los centroides dejen de moverse):
    * Se asigna cada instancia al centroide más cercano.
    * Se actualizan los centroides para que sean la media de las instancias que se les han asignado.

Ejecutemos el algoritmo K-Means para 1, 2 y 3 iteraciones, para ver cómo se mueven los centroides:
"""

random_state = 9 # 9 es óptimo, otros valores quizá no

kmeans_iter1 = KMeans(n_clusters=5, init="random", n_init=1, max_iter=1,
                      random_state=random_state)
kmeans_iter2 = KMeans(n_clusters=5, init="random", n_init=1, max_iter=2,
                      random_state=random_state)
kmeans_iter3 = KMeans(n_clusters=5, init="random", n_init=1, max_iter=3,
                      random_state=random_state)
kmeans_iter1.fit(X)
kmeans_iter2.fit(X)
kmeans_iter3.fit(X)

plt.figure(figsize=(10, 8))

plt.subplot(321)
plot_data(X)
plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')
plt.ylabel("$x_2$", rotation=0)
plt.tick_params(labelbottom=False)
plt.title("Update the centroids (initially randomly)")

plt.subplot(322)
plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False,
                         show_ylabels=False)
plt.title("Label the instances")

plt.subplot(323)
plot_decision_boundaries(kmeans_iter1, X, show_centroids=False,
                         show_xlabels=False)
plot_centroids(kmeans_iter2.cluster_centers_)

plt.subplot(324)
plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False,
                         show_ylabels=False)

plt.subplot(325)
plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)
plot_centroids(kmeans_iter3.cluster_centers_)

plt.subplot(326)
plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)

save_fig("kmeans_algorithm_plot")
plt.show()

"""**Variabilidad K-Means**

En el algoritmo K-Means original, los centroides se inicializan aleatoriamente y el algoritmo simplemente ejecuta una única iteración para mejorar gradualmente los centroides, como hemos visto anteriormente.

Sin embargo, un problema importante de este enfoque es que, si se ejecuta K-Means varias veces (o con diferentes semillas aleatorias), puede converger en soluciones muy diferentes, como se puede ver a continuación:
"""

# extra code – this cell generates and saves Figure 9–5

def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None,
                              title2=None):
    clusterer1.fit(X)
    clusterer2.fit(X)

    plt.figure(figsize=(10, 3.2))

    plt.subplot(121)
    plot_decision_boundaries(clusterer1, X)
    if title1:
        plt.title(title1)

    plt.subplot(122)
    plot_decision_boundaries(clusterer2, X, show_ylabels=False)
    if title2:
        plt.title(title2)

kmeans_rnd_init1 = KMeans(n_clusters=5, init="random", n_init=1, random_state=20, max_iter=3) #random state
kmeans_rnd_init2 = KMeans(n_clusters=5, init="random", n_init=1, random_state=250, max_iter=3) #random state

plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,
                          "Solution 1",
                          "Solution 2 (with a different random init)")

save_fig("kmeans_variability_plot")
plt.show()

good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])
kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)
kmeans.fit(X)

# extra code
plt.figure(figsize=(8, 4))
plot_decision_boundaries(kmeans, X)

"""### Inercia

Para seleccionar el mejor modelo, necesitaremos una forma de evaluar el rendimiento de un modelo K-Mean. Desafortunadamente, la agrupación es una tarea no supervisada, por lo que no tenemos los objetivos. Pero al menos podemos medir la distancia entre cada instancia y su centroide. Esta es la idea detrás de la métrica _inertia_:
"""

kmeans.inertia_

kmeans_rnd_init1.inertia_  # extra code

kmeans_rnd_init2.inertia_  # extra code